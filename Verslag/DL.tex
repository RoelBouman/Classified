\subsection{Classifiers}

\subsubsection{Deep Learning/Neural networks}

In our project, we also tried to train and test a deep neural network. We used Keras, which is a Deep Learning library for Theano and Tensorflow. The implementation of Keras models is pretty straightforward: you create a model and add layers to it via already implemented methods. In the model, we used three different layers. The first is the Dense layer, which is just a normal densely-connected neural network layer, where you have to define how many neural units you want to use. Another layer we used was the Dropout layer. This layer sets a given fraction of inputs to zero, to compensate for overfitting. The third layer we used was PReLU, or Parametric Rectified Linear Unit, which `adaptively learns the parameters of the rectifiers, and improves accuracy at negligible extra computational cost.' \cite{PReLU}

After testing the model with some input data, we implemented the model into the pipeline. The variables that can be defined for testing are the number of iterations (splits for cross-validation) and the number of epochs (separately for the initial training cycles and the final training). Furthermore, you can change the Dropout rates and Dense units, but we tried to keep these constant by using commonly used values and values that returned better results.

\bibliography{mybib}{}
\bibliographystyle{plain}

\begin{thebibliography}{9}

@article{PReLU,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on
               ImageNet Classification},
  journal   = {CoRR},
  volume    = {abs/1502.01852},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.01852},
  timestamp = {Mon, 02 Mar 2015 14:17:34 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/HeZR015},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

\end{thebibliography}

\end{document}
