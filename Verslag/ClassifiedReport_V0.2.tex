\documentclass[runningheads,a4paper]{llncs}

\usepackage[utf8]{inputenc}

\usepackage{natbib}
\bibliographystyle{apalike}

\usepackage{amssymb}
\setcounter{tocdepth}{5}
\usepackage{graphicx}
\usepackage{hyperref}

\usepackage[T1]{fontenc} % Pour que les lettres accentuées soient reconnues

\usepackage{url}
\urldef{\mailsa}\path|{alfred.hofmann, ursula.barth, ingrid.haas, frank.holzwarth,|
\urldef{\mailsb}\path|anna.kramer, leonie.kunz, christine.reiss, nicole.sator,|
\urldef{\mailsc}\path|erika.siebert-cole, peter.strasser, lncs}@springer.com|    
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\begin{document}

\mainmatter 

\title{Classified: TalkingData}

\titlerunning{Classified: TalkingData}

\author{Jordi Beernink, Thijs Werrij, Roel Bouman, Gerdriaan Mulder, Jeffrey Luppes}

\institute{Radboud University}

\authorrunning{Team Classified - TalkingData}

\toctitle{Abstract}
\tocauthor{{}}

\maketitle

\begin{abstract}
The goal of this project was to build a prediction model based on user's demographic characteristics. With as goal that millions of advertisers could pursue marketing efforts relevant to their users and catered to their preferences. The model was build with three different methods in mind as to find the most optimal algorithm to build the model with. These were LogisticRegression, XGBoost and DeepLearning. These algorithms were used in a pipeline to analyze the features of the dataset. This resulted in varying scores, where DeepLearning resulted in the best ranking of 645. This could be because of the PreLU which adaptively learns and which can improve the accuracy. 
\end{abstract}

\medskip

\begingroup
\let\clearpage\relax
\tableofcontents
\addcontentsline{toc}{section}{Introduction}
\endgroup

\medskip
\medskip

\section{Introduction}
This report details the work done on the TalkingData Competition data set by the Classified team. While the competition closed in September of 2016,  the data set remained public and open to submissions but the leaderboard was frozen.

In this report a number of approaches that were tried are discussed along with their results. Finally, some other ideas for follow-up work are discussed as well as individual contributions by team members. All of the code referenced in this report can be found in a Github repository linked to at the end of this report. 

\subsection{Data set}
The data set was collected from Kaggle and consisted of eight csv files totalling 1.2 GB. In order to generate a single data set these files were merged based on the device ID, which generated a data set of 4.2 GB. This data was fairly sparse, as many entries only had a few associated events, while others ranged in the thousands. This was true also for the phone brand types with some phones appearing only a few times while others had quite a massive share. 
\medskip

A short schema of the raw data is as follows:
\begin{itemize}
\item app\_events: \textit{event\_id, app\_id, is\_installed, is\_active}
\item app\_labels: \textit{app\_id, label\_id}
\item events: \textit{event\_id, device\_id, timestamp, coordinates}
\item gender\_age: \textit{device\_id, gender, age, group}
\item phone\_brand\_device\_model: \textit{device\_id, phone\_brand, device\_model}
\end{itemize}

\subsection{Problem Description}
\textit{"Nothing is more comforting than being greeted by your favorite drink just as you walk through the door of the corner café." -- TalkingData Competition Page } 
\smallskip
\newline
\noindent
The goal of the competition is to predict mobile behaviour based upon app usage, phone brand and location information. As an abstraction upon this, the goal is to accurately predict in which gender and age group the owner of a mobile device resides. The predictions are accepted as twelve categories: six age groups for each gender. 
\subsection{Evaluation}
Submissions are evaluated using the multi-class logarithmic loss algorithm. For each device the output is a row in a csv file with the device\_id followed by twelve probabilities between 0 and 1 corresponding to the prediction for each class.
\section{Approach}
The following section discusses the approach to this problem. 
\subsection{Pre-Processing}
The pre-processing step was the most tedious phase. After merging the .csv files together the data was still incredibly sparse: over 2/3rds of all devices had no events linked to them. With that knowledge the data files were instead combined on basis of the device properties and the installed applications. 
\subsection{Feature Extraction}
The feature extraction was the same as the pre-processing as this was done in two different methods. 
\medskip
The manual method was done by creating a co-occurrence matrix of each possible combination of features and the age gender group, gender and age. These matrices were made into histograms to visualize possible preferences of features per age gender group. These features were: Brand, Model, Apps installed and the amount of events present.
\medskip
The automatic method consisted of One Hot Encoding all the features that were mentioned in the manual method, excluding events participation\cite{OHC}. These One Hot Encoding arrays were then converted into a sparse matrix and were then dumped into pickle files. Which could be used for the pipeline. 

\section{Classifiers}
In this section the different algorithms that were used in this project will be globally explained and how they were implemented in this project. 

\subsection{Simple Classification Algorithms}
The first algorithm that was used in this project were Random Forest and LogisticRegression. 
\subsubsection{Random Forest}
To establish a baseline for a Random Forest Classifier from the Scikit-learn package was implemented, as it was persumed that it could serve as a starting point for more complex models. 

A 1000-tree model scored poorly, with the multi-class loss as reported by kaggle being 6.60. Attempts to improve upon this score were hindered by the sparse nature of the data and the RF implementation quickly became a low priority.

\subsubsection{Logistic Regression}
Another baseline model was the Logistic Regression classifier. The best working solver for the large sparse dataset was 'lbfgs'.

\medskip
\subsection{XGBoost}
\medskip
One of the methods that was used in this project was XGBOOST. Which is an acronym for Extreme Gradient Boosting. Which is a variation on the machine learning algorithm: Gradient Boosting~\cite{CT2015}. 
\medskip
Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models. It builds the model in a stage-wise fashion like other boosting methods and it generalizes them by allowing optimization of an arbitrary differentiable loss function~\cite{CT2015}.

\medskip
The difference between the two terms lies in the fact that XGBOOST uses a more regularized model formalization to control over-fitting. Which should give it a better performance~\cite{CT2014}. 
\medskip
The parameters of XGBOOST can be divided into three main aspects~\cite{AJ2016}: 

\begin{itemize}
    \item General: \textit{Which guides the overall functioning}
    \item Booster: \textit{Guides the individual booster(tree/regression) at each step}
    \item Learning \textit{Task: Guides the optimization}
\end{itemize}
\medskip
For this project the iterations, type of model (tree-based, linear), depth, weight and loss of the leaf nodes and the learning rate(known as eta) were used to get the optimal results out of the dataset~\cite{AJ2016}. Due to the lack of knowledge concerning this algorithm, it was not possible to find the optimum values for these parameters to get the optimal results.  

\subsection{Deep Learning/Neural networks}

Another algorithm that was used in this project was Deep Learning/Neural Networks. This was done by using the library of Keras with as backend Theano and Tensorflow.\medskip
The implementation of Keras models is pretty straightforward: Create a model and add layers to it via already implemented methods. The model used in this project consisted of three different layers. The first was the Dense layer, which is just a normal densely-connected neural network layer, where you have to define how many neural units you want to use. The second layer was the Dropout layer. This layer sets a given fraction of inputs to zero, to compensate for overfitting. The last layer was PReLU, or Parametric Rectified Linear Unit, which `adaptively learns the parameters of the rectifiers, and improves accuracy at negligible extra computational cost.' \cite{PReLU}\medskip

This model was first tested by using a sample of the input data to check if the algorithm worked correctly. Afterwards the algorithm was implemented into the pipeline.\medskip 

The variables that were defined for testing were the number of iterations (splits for cross-validation) and the number of epochs (separately for the initial training cycles and the final training). Furthermore, the Dropout rates and Dense units could have been changed, but because the th, but we tried to keep these constant by using commonly used values and values that returned better results.

\section{Results}
In this section the Kaggle results of the different methods will be shown and the rank in the leaderboard corresponding to that specific result.

\subsection{Simple Classifications Algorithms}
In the case of the Simple Classification algorithms the first algorithms to be tested were RandomForests and LogisticRegression. Which resulted into the following scores. \medskip 

RandomForestClassifier
\begin{itemize}
    \item Score: 3.2
    \item Position: 1667
\end{itemize}

LogisticRegression
\begin{itemize}
    \item Score: 2.265
    \item Position: 666
\end{itemize}

Other classification algorithms were tested to see if they could match with LogisticRegression. These were: DecisionTree, KNeighbors, SVC, Bagging and ExtraTrees.These performed even worse than Random Forest and were not taken into account for the rest of the paper. 

\subsection{XGBoost Results}
For the Kaggle submissions, both a linear model and the tree structure were used to test which resulted in a better score.\medskip

XGBoost Linear Model:
\begin{itemize}
    \item Score: 2.26853
    \item Position: 776
\end{itemize}

XGBoost Tree Model:
\begin{itemize}
     \item Score: 2.27968
    \item Position: 996   
\end{itemize}

\subsection{Deep Learning Results}
In the case of Deep Learning only the commonly used values for the parameters and a low amount of iterations were used to get the following result. A higher amount of iterations would have resulted in a high training time.\medskip

Deep Learning:
\begin{itemize}
    \item Score: 2.25992
    \item Position: 645
\end{itemize}

\section{Discussion}
One of the things that immediately was made clear with these results was that even with the default settings the scores of the three different algorithms was pretty good. Though because of time constraints, the parameters were unable to be optimized with an optimization algorithm. Otherwise the scores could have been better, which could have resulted into a lower score in the leaderboard. 

\section{Other ideas}
A number of concepts were considered but not explored. For features the many options with the spatial coordinates were not fully used. It could have been possible to reverse geocode the coordinate pairs to trace devices to cities and use that information as a feature. Additionally, many more features could have been involved that involve distance, density or time. 

\section{Individual Contributions}
By alphanummeric ordering:
\begin{itemize}
\item Jordi Beernink: \textit{Project leader, Pre-processing, XGBoost implementation}
\item Roel Bouman: \textit{Classification Pipeline and RF implementation}
\item Jeffrey Luppes: \textit{XGBoost implementation, Report writing}
\item Gerdriaan Mulder: \textit{Pre-processing, installation on the lilo servers}
\item Thijs Werrij - \textit{Deep Learning}
\end{itemize}
\subsection{Github Repository}
All code accompanying this report may be retrieved from \url{https://github.com/RoelBouman/Classified/tree/first_competition} where it has been bundled as a release. The README on that page alsohas extensive material on how to run the code on the lilo.science.ru.nl servers where it has been prepared for a demo. 

\bibliography{mybib}{}
\bibliographystyle{plain}
\begin{thebibliography}{9}

\bibitem{OHC}
Harris, David  and Harris, Sarah. 
\emph{: Digital design and computer architecture},  
2nd Edition,
San Francisco, Calif,
Morgan Kaufmann,
p. 129,
ISBN 978-0-12-394424-5,
24th July 2014

\bibitem{CT2015}
Chen Tianqi
\emph{: TalkingData - Linear Model on Apps and Labels},
https://www.quora.com/What-is-the-difference-between-the-R-gbm-gradient-boosting-machine-and-xgboost-extreme-gradient-boosting,
Quora,
15th September 2015.

\bibitem{CT2014}
Chen Tianqi
\emph{: Digital design and computer architecture},  
A Gentle Introduction to Gradient Boosting,
University of Washington,
22nd October 2014

\bibitem{AJ2016}
Aarshay Jain. 
\emph{: Digital design and computer architecture},  
https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/
Complete Guide to Parameter Tuning in XGBoost (with codes in Python)
1st March 2016

\bibitem{PReLU}
Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun
\emph{: Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
http://arxiv.org/abs/1502.01852,
CoRR,
2nd March 2015.  

\end{thebibliography}

\end{document}