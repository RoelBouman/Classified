\documentclass[runningheads,a4paper]{llncs}

\usepackage[utf8]{inputenc}

\usepackage{natbib}
\bibliographystyle{apalike}

\usepackage{amssymb}
\setcounter{tocdepth}{5}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{fullpage}

\usepackage[T1]{fontenc} % Pour que les lettres accentu√©es soient reconnues

\usepackage{url}
\urldef{\mailsa}\path|{alfred.hofmann, ursula.barth, ingrid.haas, frank.holzwarth,|
\urldef{\mailsb}\path|anna.kramer, leonie.kunz, christine.reiss, nicole.sator,|
\urldef{\mailsc}\path|erika.siebert-cole, peter.strasser, lncs}@springer.com|    
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\begin{document}

\mainmatter 

\title{Classified: TalkingData}

\titlerunning{Classified: TalkingData}

\author{\textit{Classified is Jordi Beernink, Thijs Werrij, Roel Bouman, Gerdriaan Mulder, and Jeffrey Luppes}}

\institute{Radboud University}

\authorrunning{Team Classified - TalkingData}

\tocauthor{{}}

\maketitle
\section{Introduction}
\noindent
This report details the work done on the TalkingData Competition data set by the Classified team. Although the competition closed in September of 2016, the data set remained public and open to submissions, but the leaderboard was frozen as the competition closed.

\subsubsection{Problem Description}
The goal of the competition is to predict mobile behaviour based upon app usage, phone brand and location information. As an abstraction upon this, the goal is to accurately predict to which gender and age group the owner of a mobile device belongs. The predictions are accepted as twelve categories: six age groups for each gender. Submissions to the competition are evaluated and ranked using the multi-class logarithmic loss metric.
\subsubsection{Data set}
The data set is hosted on Kaggle and consists of eight csv files totalling 1.2 GB. In order to generate a single data set these files were merged based on the device ID, which generated a data set of 4.2 GB. The data was fairly sparse, as many devices only had a few associated events, while others had up to thousands. This was true also for the phone brand types with some phones appearing only a few times while others had a much larger market share. 

\section{Approach}
\subsubsection{Pre-Processing}
The pre-processing step was arguably the most important phase in this competition. After merging the csv files, the data was still incredibly sparse: over two-thirds of all devices had no events linked to them. Prediction was prioritized based on the device properties and the installed applications.
\subsubsection{Feature Extraction}
All features present in the data set were aggregated into feature vectors based on the age/gender group. Additionally, some features were created by looking at what day of the week certain something was used. Using a clever One-hot-encoding, sparse matrices, and the pickle library present in Python. 
\subsection{Classifiers}
Initially a single pipeline was created using Random Forests, before being extended with other classification algorithms. As nearly all implementation followed the Scikit-learn API, the pipeline was easily extendable.
\subsubsection{Random Forests}
To establish a baseline, the Random Forest Classifier from the Scikit-learn package was used. As initial testing with Random Forests yielded a score below that of the benchmark submission, further analysis using Random Forests was abandoned early on.
\subsubsection{Logistic Regression}
The Logistic Regression classifier from the Scikit-learn package in Python was used for classification. This provided quite decent results, especially when compared to other submissions on Kaggle. The best working solver for the large sparse data set was Limited-memory BFGS. 
\subsubsection{XGBoost}
Extreme Gradient Boosting~\cite{CT2015} was a more novel method used in classification. While computational intense, and harder to use, it was well suited for the problem. \medskip
\subsubsection{Deep Learning/Neural networks}
With the use of Keras, a Neural Network library with a Tensorflow back-end, a neural network model was made. The network consists of several dense layers, dropout layers, and layers that use the well known Parametric Rectified Linear Unit (PReLU) \cite{PReLU}. 


\section{Results}
The results of the classifiers are displayed in the following table. 

\begin{center}
    \begin{tabular}{| c | c | c|}
    \hline 
    Classifier & Mult-class logloss &  Kaggle rank (private leaderboard)\\ \hline
    Benchmark submission & 2.48491 & 1557 \\ \hline
    Random Forests & 3.261 & 1667 \\ \hline
    XGBoost tree & 2.280 & 996 \\ \hline
    XGBoost Linear & 2.269 & 776 \\ \hline
    Logistic Regression & 2.265 & 666 \\ \hline
    Neural Network (Keras) & 2.251 & 645 \\ \hline
    \end{tabular}
\end{center}

\section{Discussion}
The neural network outperformed the other classifiers by a small margin, but the relatively simple logistic regression model did come extremely close. This is still better than over 60\% of the submissions. 

\section{Other ideas}
The main focus point for improvement seems to be the pre-processing and the selection of features. A large amount of optimization of the classification was done, without impacting the score much.

A number of concepts for features were considered but not explored. Perhaps classification can be improved by mapping users to urban areas or specific cities. Additionally, many more features could have been involved that involve distance, density or time. Sadly, the contest did not allow for data enrichment with data outside of the competition.

\section{Individual Contributions}
By alphanumeric ordering:
\begin{itemize}
\item Jordi Beernink: \textit{Project leader, Pre-processing, XGBoost implementation}
\item Roel Bouman: \textit{Classification Pipeline, Random Forests and Logistic Regression}
\item Jeffrey Luppes: \textit{XGBoost implementation, Report writing}
\item Gerdriaan Mulder: \textit{Pre-processing, installation on the lilo servers}
\item Thijs Werrij - \textit{Deep Learning}
\end{itemize}
\subsection{Github Repository}
All code accompanying this report may be retrieved from \url{https://github.com/RoelBouman/Classified/tree/first_competition} where it has been bundled as a release. The README on that page also has extensive material on how to run the code on the lilo.science.ru.nl servers where it has been prepared for a demo. 

\bibliography{mybib}{}
\bibliographystyle{plain}
\begin{thebibliography}{9}

\bibitem{CT2015}
Chen Tianqi
\emph{: TalkingData - Linear Model on Apps and Labels},
https://www.quora.com/What-is-the-difference-between-the-R-gbm-gradient-boosting-machine-and-xgboost-extreme-gradient-boosting,
Quora,
15th September 2015.

\bibitem{PReLU}
Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun
\emph{: Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
http://arxiv.org/abs/1502.01852,
CoRR,
2nd March 2015.  

\end{thebibliography}

\end{document}
